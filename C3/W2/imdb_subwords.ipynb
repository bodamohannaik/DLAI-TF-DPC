{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_subwords.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPwcol77enYhM464juNXQ9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bodamohannaik/DLAI-TF-DPC/blob/master/C3/W2/imdb_subwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "Qr3b0D2dTJCK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "1NnKNsysTBOF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SubwordTextEncoder"
      ],
      "metadata": {
        "id": "y9ZbKb1PUnT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentences = [\n",
        "                    \"python is a high level language\",\n",
        "                    \"python is used in automation development\",\n",
        "                    \"python is used for AI and ML\",\n",
        "                    \"python is also used for web development\",\n",
        "                    \"python is the most used language\"\n",
        "]"
      ],
      "metadata": {
        "id": "7PvCCUXzUl0S"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subwordtextencoder = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_generator=sample_sentences,\n",
        "                                                                               target_vocab_size=268, max_subword_length=20,)"
      ],
      "metadata": {
        "id": "yhGRdb5LVNhr"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subwordtextencoder.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsXveH6SYGRg",
        "outputId": "575c11e4-6d64-4f13-cba1-044846211bd0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "262"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subwordtextencoder.subwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ufwi3bQVkhZ",
        "outputId": "0f09e6a8-7f82-42ad-b292-2e103c050356"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['python_', 'is_', 'used_', 'evel', 'an']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subwordtextencoder.encode(sample_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVRe7HviaypL",
        "outputId": "85c8af71-c5ee-4ade-c8a1-a54e12bce804"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 2,\n",
              " 103,\n",
              " 38,\n",
              " 110,\n",
              " 111,\n",
              " 109,\n",
              " 110,\n",
              " 38,\n",
              " 114,\n",
              " 4,\n",
              " 38,\n",
              " 114,\n",
              " 5,\n",
              " 109,\n",
              " 123,\n",
              " 103,\n",
              " 109,\n",
              " 107]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMDB subword 8K"
      ],
      "metadata": {
        "id": "32Hd5rMLbiyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subwords_imdb, subwords_info = tfds.load('imdb_reviews/subwords8k', as_supervised=True, with_info=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0UoKXEbbnwi",
        "outputId": "769921ff-540c-4191-bf23-af8a89f13b9b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)"
      ],
      "metadata": {
        "id": "AwziwSjdcHEd"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCOu2ZUUc1ot",
        "outputId": "bd7d6c63-83b5-40be-9d52-ad0daf237ce5"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    version=1.0.0,\n",
              "    description='Large Movie Review Dataset.\n",
              "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        'text': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=100000,\n",
              "    splits={\n",
              "        'test': 25000,\n",
              "        'train': 25000,\n",
              "        'unsupervised': 50000,\n",
              "    },\n",
              "    supervised_keys=('text', 'label'),\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item, label in imdb['train'].take(2):\n",
        "  print(\"*\"*120)\n",
        "  print(item)\n",
        "  print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuqBB9AUdGFI",
        "outputId": "861d1e44-cfe5-47e7-afd3-22c48338958a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************************************************************************************************\n",
            "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "************************************************************************************************************************\n",
            "tf.Tensor(b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.', shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subwords_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ct3TYsZ2dViy",
        "outputId": "510f190d-3835-43f7-f577-8beb4f819401"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    version=1.0.0,\n",
              "    description='Large Movie Review Dataset.\n",
              "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        'text': Text(shape=(None,), dtype=tf.int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
              "    }),\n",
              "    total_num_examples=100000,\n",
              "    splits={\n",
              "        'test': 25000,\n",
              "        'train': 25000,\n",
              "        'unsupervised': 50000,\n",
              "    },\n",
              "    supervised_keys=('text', 'label'),\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item, label in subwords_imdb['train'].take(2):\n",
        "  print(\"*\"*120)\n",
        "  print(item)\n",
        "  print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tlsu4ubdaS5",
        "outputId": "7f6f5b06-db73-4c33-da3d-d93629a90e86"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************************************************************************************************\n",
            "tf.Tensor(\n",
            "[  62   18   41  604  927   65    3  644 7968   21   35 5096   36   11\n",
            "   43 2948 5240  102   50  681 7862 1244    3 3266   29  122  640    2\n",
            "   26   14  279  438   35   79  349  384   11 1991    3  492   79  122\n",
            "  188  117   33 4047 4531   14   65 7968    8 1819 3947    3   62   27\n",
            "    9   41  577 5044 2629 2552 7193 7961 3642    3   19  107 3903  225\n",
            "   85  198   72    1 1512  738 2347  102 6245    8   85  308   79 6936\n",
            " 7961   23 4981 8044    3 6429 7961 1141 1335 1848 4848   55 3601 4217\n",
            " 8050    2    5   59 3831 1484 8040 7974  174 5773   22 5240  102   18\n",
            "  247   26    4 3903 1612 3902  291   11    4   27   13   18 4092 4008\n",
            " 7961    6  119  213 2774    3   12  258 2306   13   91   29  171   52\n",
            "  229    2 1245 5790  995 7968    8   52 2948 5240 8039 7968    8   74\n",
            " 1249    3   12  117 2438 1369  192   39 7975], shape=(163,), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "************************************************************************************************************************\n",
            "tf.Tensor(\n",
            "[  12   31   93  867    7 1256 6585 7961  421  365    2   26   14    9\n",
            "  988 1089    7    4 6728    6  276 5760 2587    2   81 6118 8029    2\n",
            "  139 1892 7961    5 5402  246   25    1 1771  350    5  369   56 5397\n",
            "  102    4 2547    3 4001   25   14 7822  209   12 3531 6585 7961   99\n",
            "    1   32   18 4762    3   19  184 3223   18 5855 1045    3 4232 3337\n",
            "   64 1347    5 1190    3 4459    8  614    7 3129    2   26   22   84\n",
            " 7020    6   71   18 4924 1160  161   50 2265    3   12 3983    2   12\n",
            "  264   31 2545  261    6    1   66    2   26  131  393    1 5846    6\n",
            "   15    5  473   56  614    7 1470    6  116  285 4755 2088 7961  273\n",
            "  119  213 3414 7961   23  332 1019    3   12 7667  505   14   32   44\n",
            "  208 7975], shape=(142,), dtype=int64)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subwords_tokenizer= subwords_info.features['text'].encoder"
      ],
      "metadata": {
        "id": "eBndJSuye1ph"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item, label in subwords_imdb['train'].take(2):\n",
        "  print(\"*\"*120)\n",
        "  print(subwords_tokenizer.decode(item))\n",
        "  print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIGUNXKAfoO9",
        "outputId": "d12fed9b-2479-4764-86f6-8b9cb9e4c569"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************************************************************************************************************\n",
            "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "************************************************************************************************************************\n",
            "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Word Tokenizer require large vocabulary size to avoid OOV on train set , inspite of having large vocabulary aslo, on train samples, we get OOV\n",
        "* Unlike word Tokenizer , Subword Tokenizer with small vocabulary will avoid OOV . But only downside is the more tokens for a given text"
      ],
      "metadata": {
        "id": "rfX1FEk5ufi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueral Network"
      ],
      "metadata": {
        "id": "lC5eZGlMvA9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = subwords_imdb['train']\n",
        "test_sequences = subwords_imdb['test']"
      ],
      "metadata": {
        "id": "sClurCPnwT3J"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiVHw6zFxR_c",
        "outputId": "bf10fae7-0b0d-410b-f335-fff44ca3f822"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.PrefetchDataset"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = train_sequences.padded_batch(batch_size=32)\n",
        "test_sequences = test_sequences.padded_batch(batch_size=32)"
      ],
      "metadata": {
        "id": "1XIA0EsLx4h4"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential(\n",
        "    layers = [\n",
        "              tf.keras.layers.Input(shape=(None,)),\n",
        "              tf.keras.layers.Embedding(input_dim=subwords_tokenizer.vocab_size, output_dim=64),\n",
        "              tf.keras.layers.GlobalAveragePooling1D(),\n",
        "              tf.keras.layers.Dense(units=6, activation='relu'),\n",
        "              tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZXGGnUOyOCk",
        "outputId": "fbae785f-5468-473c-b9f4-621b32fac6e9"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          523840    \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 64)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 390       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 7         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 524,237\n",
            "Trainable params: 524,237\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compile\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "# train\n",
        "model.fit(train_sequences, epochs=10, validation_data=test_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SutTE2jzb_f",
        "outputId": "5cd16ad7-5664-4085-84c1-a201101ec8e7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 21s 26ms/step - loss: 0.6303 - accuracy: 0.6678 - val_loss: 0.5061 - val_accuracy: 0.8028\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.3942 - accuracy: 0.8585 - val_loss: 0.3541 - val_accuracy: 0.8675\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.2897 - accuracy: 0.8952 - val_loss: 0.3188 - val_accuracy: 0.8784\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.2435 - accuracy: 0.9112 - val_loss: 0.3092 - val_accuracy: 0.8812\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.2139 - accuracy: 0.9224 - val_loss: 0.3089 - val_accuracy: 0.8826\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.1919 - accuracy: 0.9310 - val_loss: 0.3142 - val_accuracy: 0.8814\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.1741 - accuracy: 0.9375 - val_loss: 0.3236 - val_accuracy: 0.8800\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.1593 - accuracy: 0.9434 - val_loss: 0.3368 - val_accuracy: 0.8766\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.1465 - accuracy: 0.9484 - val_loss: 0.3519 - val_accuracy: 0.8750\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.1357 - accuracy: 0.9528 - val_loss: 0.3684 - val_accuracy: 0.8734\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f59b33bb690>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    }
  ]
}